---
layout: 'publication'
title: "Lifelong Inverse Reinforcement Learning"
collection: publications
type: 'conference'
permalink: /publication/2018-lifelong-inverse-rl
excerpt: 'We introduced the problem of lifelong learning from demonstrations, and created an efficient lifelong inverse reinforcement learning (ELIRL) algorithm.'
date: 2018-12-01
venue: 'Advances in Neural Information Processing Systems (NeurIPS)'
paperurl: 'http://papers.nips.cc/paper/7702-lifelong-inverse-reinforcement-learning.pdf'
authors: '<strong>Jorge Mendez-Mendez</strong>, Shashank Shivkumar, <a href="https://seas.upenn.edu/~eeaton/">Eric Eaton</a>'
thumbnail: 'LifelongLfD.jpg'
codeurl: 'https://github.com/Lifelong-ML/ELIRL.git'
videourl: 'https://youtu.be/Of5OyuOrePw'
posterurl: 'https://www.seas.upenn.edu/~eeaton/papers/Mendez2018Lifelong-poster.pdf'
abstract: 'Methods for learning from demonstration (LfD) have shown success in acquiring behavior policies by imitating a user. However, even for a single task, LfD may require numerous demonstrations. For versatile agents that must learn many tasks via demonstration, this process would substantially burden the user if each task were learned in isolation. To address this challenge, we introduce the novel problem of lifelong learning from demonstration, which allows the agent to continually build upon knowledge learned from previously demonstrated tasks to accelerate the learning of new tasks, reducing the amount of demonstrations required. As one solution to this problem, we propose the first lifelong learning approach to inverse reinforcement learning, which learns consecutive tasks via demonstration, continually transferring knowledge between tasks to improve performance.'
bibtex: '@inproceedings{mendez2018lifelong,
<br> author = {Mendez-Mendez, Jorge and Shivkumar, Shashank and Eaton, Eric},
<br> booktitle = {Advances in Neural Information Processing Systems 31 (NeurIPS-18)},
<br> title = {Lifelong Inverse Reinforcement Learning},
<br> year = {2018}
<br>}'
---
